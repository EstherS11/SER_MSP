#!/usr/bin/env python3
# completely_fixed_cloud_run.py - å®Œå…¨ä¿®å¤ç‰ˆäº‘ç«¯è¿è¡Œè„šæœ¬

import sys
import os
import json
import subprocess
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import numpy as np
from pathlib import Path
from collections import Counter
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import f1_score, classification_report
from tqdm import tqdm
import logging

# ============================================================================
# äº‘ç«¯é›†ç¾¤é…ç½®
# ============================================================================

DATA_ROOT = "/data/user_data/esthers/SER_MSP"
BASELINE_DIR = "/data/user_data/esthers/SER_MSP/baseline"
AUDIO_DIR = "/data/user_data/esthers/SER_MSP/DATA/Audios"

JSON_FILES = {
    'train': f"{DATA_ROOT}/msp_train_10class.json",
    'valid': f"{DATA_ROOT}/msp_valid_10class.json", 
    'test': f"{DATA_ROOT}/msp_test_10class.json"
}

# ============================================================================
# ä¿®å¤1ï¼šè§£å†³éŸ³é¢‘æ–‡ä»¶è·¯å¾„é—®é¢˜
# ============================================================================

def fix_audio_paths(json_file, audio_dir):
    """ä¿®å¤JSONæ–‡ä»¶ä¸­çš„éŸ³é¢‘è·¯å¾„"""
    print(f"ğŸ”§ Fixing audio paths in {json_file}...")
    
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    audio_path = Path(audio_dir)
    fixed_count = 0
    missing_count = 0
    
    for utt_id, info in data.items():
        original_path = info['wav']
        
        # å¦‚æœåŸè·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨AUDIO_DIRä¸­æŸ¥æ‰¾
        if not os.path.exists(original_path):
            # ä»åŸè·¯å¾„æå–æ–‡ä»¶å
            filename = os.path.basename(original_path)
            new_path = audio_path / filename
            
            if new_path.exists():
                info['wav'] = str(new_path)
                fixed_count += 1
            else:
                missing_count += 1
        else:
            fixed_count += 1
    
    print(f"âœ… Fixed {fixed_count} paths, {missing_count} still missing")
    return data, fixed_count, missing_count

def prepare_fixed_data(output_dir="data"):
    """å‡†å¤‡ä¿®å¤åçš„æ•°æ®"""
    print("ğŸ”§ Preparing data with fixed paths...")
    
    emotion_map = {
        'N': 0, 'H': 1, 'S': 2, 'A': 3, 'F': 4,
        'D': 5, 'U': 6, 'C': 7, 'O': 8, 'X': 9
    }
    
    emotion_names = [
        "neutral", "happy", "sad", "angry", "fear",
        "disgust", "surprise", "contempt", "other", "unknown"
    ]
    
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    total_stats = {}
    
    for split, json_file in JSON_FILES.items():
        print(f"\nğŸ”§ Processing {split} set...")
        
        if not Path(json_file).exists():
            print(f"âŒ {json_file} not found")
            continue
        
        # ä¿®å¤éŸ³é¢‘è·¯å¾„
        data, fixed_count, missing_count = fix_audio_paths(json_file, AUDIO_DIR)
        
        split_dir = output_path / split
        split_dir.mkdir(exist_ok=True)
        
        valid_samples = []
        emotion_counts = {}
        
        # æ”¶é›†æœ‰æ•ˆæ ·æœ¬
        for utt_id, info in data.items():
            wav_path = info['wav']
            emotion = info['emo']
            
            # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨ä¸”æ ‡ç­¾æœ‰æ•ˆ
            if os.path.exists(wav_path) and emotion in emotion_map:
                valid_samples.append((utt_id, wav_path, emotion_map[emotion]))
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        print(f"âœ… {split}: {len(valid_samples)} valid samples from {len(data)} total")
        
        # å†™å…¥æ–‡ä»¶
        with open(split_dir / "speech.scp", 'w') as scp_f, \
             open(split_dir / "emotion.txt", 'w') as emo_f, \
             open(split_dir / "utt2spk", 'w') as spk_f:
            
            for utt_id, wav_path, emotion_idx in valid_samples:
                scp_f.write(f"{utt_id} {wav_path}\n")
                emo_f.write(f"{utt_id} {emotion_idx}\n")
                
                # ç®€å•çš„speaker ID
                speaker_id = '_'.join(utt_id.split('_')[:2])
                spk_f.write(f"{utt_id} {speaker_id}\n")
        
        total_stats[split] = {
            'total_samples': len(data),
            'valid_samples': len(valid_samples),
            'missing_files': len(data) - len(valid_samples),
            'emotion_distribution': emotion_counts
        }
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    with open(output_path / "stats.json", 'w') as f:
        json.dump(total_stats, f, indent=2)
    
    print(f"\nâœ… Data prepared in {output_dir}/")
    print("ğŸ“Š Statistics:")
    for split, stats in total_stats.items():
        print(f"  {split}: {stats['valid_samples']}/{stats['total_samples']} valid samples")
    
    return total_stats

# ============================================================================
# ä¿®å¤2ï¼šç®€åŒ–çš„ESP-neté…ç½®
# ============================================================================

def create_fixed_espnet_config():
    """åˆ›å»ºä¿®å¤åçš„ESP-neté…ç½®"""
    
    config = """# Fixed ESP-net ASR configuration
batch_type: numel
batch_size: 16
max_epoch: 30
patience: 5
seed: 42
num_workers: 4
log_interval: 100
grad_clip: 5.0
accum_grad: 1

# Optimizer
optim: adamw
optim_conf:
  lr: 0.0001
  weight_decay: 0.001

# Scheduler
scheduler: warmuplr
scheduler_conf:
  warmup_steps: 1000

# Model configuration
model: espnet
model_conf:
  ctc_weight: 0.3
  lsm_weight: 0.1
  length_normalized_loss: false
  
# Encoder
encoder: conformer
encoder_conf:
  output_size: 256
  attention_heads: 4
  linear_units: 1024
  num_blocks: 6
  dropout_rate: 0.1
  attention_dropout_rate: 0.0
  input_layer: conv2d
  normalize_before: true
  
# Decoder
decoder: transformer
decoder_conf:
  attention_heads: 4
  linear_units: 1024
  num_blocks: 3
  dropout_rate: 0.1

# Frontend
frontend: default
frontend_conf:
  n_fft: 512
  win_length: 400
  hop_length: 160

# SpecAugment
specaug: specaug
specaug_conf:
  apply_time_warp: true
  time_warp_window: 5
  apply_freq_mask: true
  freq_mask_width_range: [0, 30]
  num_freq_mask: 2
  apply_time_mask: true
  time_mask_width_range: [0, 40]
  num_time_mask: 2

# Best model criterion
best_model_criterion:
  - ["valid", "acc", "max"]
  - ["valid", "loss", "min"]

# Other settings
resume: true
keep_nbest_models: 3
use_tensorboard: true
"""
    
    with open("fixed_espnet_config.yaml", 'w') as f:
        f.write(config)
    
    print("âœ… Created fixed_espnet_config.yaml")

# ============================================================================
# ä¿®å¤3ï¼šå®Œæ•´çš„PyTorch SERå®ç°
# ============================================================================

class SimpleSERDataset(Dataset):
    """ä¿®å¤åçš„SERæ•°æ®é›†"""
    def __init__(self, data_dir, split='train', max_length=16000*5):
        self.max_length = max_length
        self.data = []
        
        # è¯»å–æ•°æ®
        speech_file = Path(data_dir) / split / "speech.scp"
        emotion_file = Path(data_dir) / split / "emotion.txt"
        
        if not speech_file.exists() or not emotion_file.exists():
            raise FileNotFoundError(f"Data files not found in {data_dir}/{split}")
        
        speech_data = {}
        with open(speech_file, 'r') as f:
            for line in f:
                parts = line.strip().split(None, 1)
                if len(parts) == 2:
                    utt_id, path = parts
                    speech_data[utt_id] = path
        
        with open(emotion_file, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) == 2:
                    utt_id, emotion = parts
                    if utt_id in speech_data:
                        self.data.append((speech_data[utt_id], int(emotion)))
        
        print(f"ğŸ“Š {split} dataset: {len(self.data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        wav_path, emotion = self.data[idx]
        
        try:
            # åŠ è½½éŸ³é¢‘
            waveform, sr = torchaudio.load(wav_path)
            
            # é‡é‡‡æ ·åˆ°16kHz
            if sr != 16000:
                resampler = torchaudio.transforms.Resample(sr, 16000)
                waveform = resampler(waveform)
            
            # è½¬ä¸ºå•å£°é“
            if waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            
            # å¤„ç†é•¿åº¦
            if waveform.shape[1] > self.max_length:
                waveform = waveform[:, :self.max_length]
            else:
                pad_length = self.max_length - waveform.shape[1]
                waveform = F.pad(waveform, (0, pad_length))
                
            return waveform.squeeze(0), emotion
            
        except Exception as e:
            print(f"âš ï¸  Error loading {wav_path}: {e}")
            return torch.zeros(self.max_length), 0

class SimpleSERModel(nn.Module):
    """æ”¹è¿›çš„SERæ¨¡å‹"""
    def __init__(self, num_classes=10):
        super().__init__()
        
        # CNNç‰¹å¾æå–å™¨
        self.features = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šç²—ç²’åº¦ç‰¹å¾
            nn.Conv1d(1, 64, kernel_size=80, stride=16),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.1),
            
            # ç¬¬äºŒå±‚ï¼šä¸­ç­‰ç²’åº¦ç‰¹å¾
            nn.Conv1d(64, 128, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.1),
            
            # ç¬¬ä¸‰å±‚ï¼šç»†ç²’åº¦ç‰¹å¾
            nn.Conv1d(128, 256, kernel_size=3, stride=2),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.1),
            
            # å…¨å±€å¹³å‡æ± åŒ–
            nn.AdaptiveAvgPool1d(1)
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        # x: (batch, time)
        x = x.unsqueeze(1)  # (batch, 1, time)
        x = self.features(x)  # (batch, 256, 1)
        x = x.squeeze(-1)  # (batch, 256)
        x = self.classifier(x)  # (batch, num_classes)
        return x

def train_pytorch_ser():
    """è®­ç»ƒPyTorch SERæ¨¡å‹"""
    print("ğŸš€ Training PyTorch SER Model...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  Using device: {device}")
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å‡†å¤‡å¥½
    if not Path("data/train/speech.scp").exists():
        print("âŒ Data not prepared. Running data preparation first...")
        stats = prepare_fixed_data()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
        if all(stats[split]['valid_samples'] == 0 for split in ['train', 'valid']):
            print("âŒ No valid samples found after data preparation!")
            print("ğŸ” Please check:")
            print(f"  1. Audio files are in: {AUDIO_DIR}")
            print(f"  2. JSON files contain correct paths")
            return False
    
    # æ•°æ®åŠ è½½
    try:
        train_dataset = SimpleSERDataset("data", "train")
        valid_dataset = SimpleSERDataset("data", "valid")
        
        if len(train_dataset) == 0 or len(valid_dataset) == 0:
            print("âŒ Empty datasets!")
            return False
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=8, 
            shuffle=True, 
            num_workers=2,
            pin_memory=True if device.type == 'cuda' else False
        )
        valid_loader = DataLoader(
            valid_dataset, 
            batch_size=8, 
            shuffle=False, 
            num_workers=2,
            pin_memory=True if device.type == 'cuda' else False
        )
        
        print(f"ğŸ“Š Dataset sizes - Train: {len(train_dataset)}, Valid: {len(valid_dataset)}")
        
    except Exception as e:
        print(f"âŒ Error loading data: {e}")
        return False
    
    # æ¨¡å‹è®¾ç½®
    model = SimpleSERModel(num_classes=10).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)
    
    print(f"ğŸ”§ Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    best_macro_f1 = 0.0
    patience_counter = 0
    max_patience = 5
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(20):
        print(f"\nğŸ“ˆ Epoch {epoch+1}/20")
        
        # === è®­ç»ƒé˜¶æ®µ ===
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc="Training", leave=False)
        for batch_idx, (audio, labels) in enumerate(train_pbar):
            audio, labels = audio.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(audio)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            train_pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*train_correct/train_total:.2f}%'
            })
        
        train_acc = 100. * train_correct / train_total
        avg_train_loss = train_loss / len(train_loader)
        
        # === éªŒè¯é˜¶æ®µ ===
        model.eval()
        val_loss = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            val_pbar = tqdm(valid_loader, desc="Validation", leave=False)
            for audio, labels in val_pbar:
                audio, labels = audio.to(device), labels.to(device)
                outputs = model(audio)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                val_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
        
        # è®¡ç®—æŒ‡æ ‡
        val_acc = 100. * np.mean(np.array(all_preds) == np.array(all_labels))
        avg_val_loss = val_loss / len(valid_loader)
        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        
        # æ‰“å°ç»“æœ
        print(f"ğŸ‹ï¸  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"ğŸ“Š Valid Loss: {avg_val_loss:.4f}, Valid Acc: {val_acc:.2f}%")
        print(f"ğŸ¯ Macro-F1: {macro_f1:.4f}, LR: {current_lr:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if macro_f1 > best_macro_f1:
            best_macro_f1 = macro_f1
            patience_counter = 0
            
            # ä¿å­˜æ¨¡å‹
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_macro_f1': best_macro_f1,
                'val_acc': val_acc,
                'val_loss': avg_val_loss
            }
            torch.save(checkpoint, "best_ser_model.pth")
            print(f"ğŸ’¾ New best model saved! Macro-F1: {best_macro_f1:.4f}")
            
        else:
            patience_counter += 1
            print(f"â³ Patience: {patience_counter}/{max_patience}")
            
            if patience_counter >= max_patience:
                print("ğŸ›‘ Early stopping triggered!")
                break
    
    # æœ€ç»ˆè¯„ä¼°
    print(f"\nğŸ‰ Training completed!")
    print(f"ğŸ† Best Macro-F1: {best_macro_f1:.4f}")
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    if len(set(all_labels)) > 1:
        emotion_names = [
            "Neutral", "Happy", "Sad", "Angry", "Fear",
            "Disgust", "Surprise", "Contempt", "Other", "Unknown"
        ]
        try:
            report = classification_report(
                all_labels, 
                all_preds,
                target_names=emotion_names[:len(set(all_labels))],
                zero_division=0,
                digits=4
            )
            print(f"\nğŸ“‹ Final Classification Report:\n{report}")
        except:
            print("âš ï¸  Could not generate detailed classification report")
    
    return True

# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒ"""
    print("ğŸ” Checking environment...")
    
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    checks = [
        (DATA_ROOT, "Data root"),
        (AUDIO_DIR, "Audio directory"),
    ]
    
    for path, name in checks:
        if Path(path).exists():
            print(f"âœ… {name}: {path}")
        else:
            print(f"âŒ {name} not found: {path}")
            return False
    
    # æ£€æŸ¥JSONæ–‡ä»¶
    for split, json_path in JSON_FILES.items():
        if Path(json_path).exists():
            print(f"âœ… {split} JSON: {json_path}")
        else:
            print(f"âŒ {split} JSON not found: {json_path}")
            return False
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"âœ… CUDA: {torch.cuda.get_device_name()}")
    else:
        print("âš ï¸  No CUDA, using CPU")
    
    return True

def main():
    print("ğŸŒ Completely Fixed Cloud SER Training")
    print("=" * 60)
    print(f"ğŸ“ Data root: {DATA_ROOT}")
    print(f"ğŸµ Audio dir: {AUDIO_DIR}")
    print("=" * 60)
    
    # 1. ç¯å¢ƒæ£€æŸ¥
    if not check_environment():
        print("âŒ Environment check failed")
        return
    
    # 2. æ•°æ®å‡†å¤‡
    print("\nğŸ”§ Step 1: Data Preparation")
    try:
        stats = prepare_fixed_data()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        train_samples = stats.get('train', {}).get('valid_samples', 0)
        valid_samples = stats.get('valid', {}).get('valid_samples', 0)
        
        if train_samples == 0 or valid_samples == 0:
            print("âŒ Insufficient valid data found!")
            print("ğŸ” Troubleshooting suggestions:")
            print(f"  1. Check if audio files exist in: {AUDIO_DIR}")
            print(f"  2. Verify JSON file paths are correct")
            print(f"  3. Ensure audio file extensions match (.wav)")
            return
        
        print(f"âœ… Found {train_samples} training samples, {valid_samples} validation samples")
        
    except Exception as e:
        print(f"âŒ Data preparation failed: {e}")
        return
    
    # 3. åˆ›å»ºé…ç½®æ–‡ä»¶
    print("\nğŸ”§ Step 2: Configuration")
    create_fixed_espnet_config()
    
    # 4. è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ Step 3: Model Training")
    
    try:
        if train_pytorch_ser():
            print("\nğŸ‰ Training completed successfully!")
            print("\nğŸ“Š Results:")
            print("  - Model: best_ser_model.pth")
            print("  - Logs: Training progress shown above")
            print("\nğŸ¯ Next steps:")
            print("  - Test the model on test set")
            print("  - Analyze per-class performance")
            print("  - Consider hyperparameter tuning")
        else:
            print("\nâŒ Training failed")
            
    except Exception as e:
        print(f"\nâŒ Training error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()