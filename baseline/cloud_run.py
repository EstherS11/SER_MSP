#!/usr/bin/env python3
# cloud_run.py - äº‘ç«¯é›†ç¾¤ä¸“ç”¨è¿è¡Œè„šæœ¬

import sys
import os
import json
import subprocess
from pathlib import Path

# ============================================================================
# äº‘ç«¯é›†ç¾¤é…ç½® - æ ¹æ®ä½ çš„å®é™…è·¯å¾„
# ============================================================================

# ğŸ”§ ä½ çš„å®é™…æ•°æ®è·¯å¾„
DATA_ROOT = "/data/user_data/esthers/SER_MSP"
BASELINE_DIR = "/data/user_data/esthers/SER_MSP/baseline"  # è¿™ä¸ªç›®å½•å¯èƒ½ä¸å­˜åœ¨ï¼Œæˆ‘ä»¬ä¼šåˆ›å»º
AUDIO_DIR = "/data/user_data/esthers/SER_MSP/DATA/Audios"

# JSONæ–‡ä»¶è·¯å¾„ï¼ˆç›´æ¥åœ¨SER_MSPç›®å½•ä¸‹ï¼‰
JSON_FILES = {
    'train': f"{DATA_ROOT}/msp_train_10class.json",
    'valid': f"{DATA_ROOT}/msp_valid_10class.json", 
    'test': f"{DATA_ROOT}/msp_test_10class.json"
}

# ============================================================================
# äº‘ç«¯ç¯å¢ƒæ£€æŸ¥å’Œè®¾ç½®
# ============================================================================

def setup_cloud_environment():
    """è®¾ç½®äº‘ç«¯ç¯å¢ƒ"""
    project_root = Path(__file__).parent.absolute()
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # åˆ›å»º__init__.pyæ–‡ä»¶
    init_file = project_root / "__init__.py"
    if not init_file.exists():
        init_file.touch()
        print("âœ… Created __init__.py")
    
    # åˆ›å»ºbaselineç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    baseline_path = Path(BASELINE_DIR)
    baseline_path.mkdir(parents=True, exist_ok=True)
    print(f"âœ… Baseline directory: {BASELINE_DIR}")
    
    return project_root

def check_cloud_data():
    """æ£€æŸ¥äº‘ç«¯æ•°æ®ç»“æ„"""
    print("ğŸ” Checking cloud data structure...")
    
    # æ£€æŸ¥éŸ³é¢‘ç›®å½•
    audio_path = Path(AUDIO_DIR)
    if not audio_path.exists():
        print(f"âŒ Audio directory not found: {AUDIO_DIR}")
        return False
    
    audio_files = list(audio_path.glob("*.wav"))
    print(f"âœ… Found {len(audio_files)} audio files in {AUDIO_DIR}")
    
    # æ£€æŸ¥JSONæ–‡ä»¶
    for split, json_path in JSON_FILES.items():
        if not Path(json_path).exists():
            print(f"âŒ Missing JSON file: {json_path}")
            return False
        
        # æ£€æŸ¥JSONæ ¼å¼
        try:
            with open(json_path, 'r') as f:
                data = json.load(f)
            print(f"âœ… {split}: {len(data)} samples in {json_path}")
            
            # éªŒè¯æ•°æ®æ ¼å¼
            first_key = next(iter(data))
            first_item = data[first_key]
            if 'wav' not in first_item or 'emo' not in first_item:
                print(f"âŒ Invalid JSON format in {json_path}")
                return False
                
        except Exception as e:
            print(f"âŒ Error reading {json_path}: {e}")
            return False
    
    return True

def create_cloud_data_prep():
    """åˆ›å»ºé€‚é…äº‘ç«¯æ•°æ®ç»“æ„çš„æ•°æ®å‡†å¤‡è„šæœ¬"""
    
    data_prep_code = '''#!/usr/bin/env python3
# cloud_data_prep.py - äº‘ç«¯æ•°æ®å‡†å¤‡è„šæœ¬

import json
import os
import numpy as np
from pathlib import Path
from collections import Counter

def prepare_cloud_msp_data(output_dir="data"):
    """å‡†å¤‡äº‘ç«¯MSP-PODCASTæ•°æ®"""
    
    # äº‘ç«¯è·¯å¾„é…ç½®
    JSON_FILES = {
        'train': "/data/user_data/esthers/SER_MSP/msp_train_10class.json",
        'valid': "/data/user_data/esthers/SER_MSP/msp_valid_10class.json", 
        'test': "/data/user_data/esthers/SER_MSP/msp_test_10class.json"
    }
    
    # MSP-PODCASTæƒ…æ„Ÿæ ‡ç­¾æ˜ å°„
    emotion_map = {
        'N': 0, 'H': 1, 'S': 2, 'A': 3, 'F': 4,
        'D': 5, 'U': 6, 'C': 7, 'O': 8, 'X': 9
    }
    
    emotion_names = [
        "neutral", "happy", "sad", "angry", "fear",
        "disgust", "surprise", "contempt", "other", "unknown"
    ]
    
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    print("ğŸ”§ Preparing ESP-net format data for cloud MSP-PODCAST...")
    
    stats = {}
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for split, json_file in JSON_FILES.items():
        print(f"\\nProcessing {split} set...")
        
        if not Path(json_file).exists():
            print(f"âŒ {json_file} not found")
            continue
            
        with open(json_file, 'r') as f:
            data = json.load(f)
        
        split_dir = output_path / split
        split_dir.mkdir(exist_ok=True)
        
        valid_count = 0
        durations = []
        emotion_counts = {}
        missing_files = []
        
        # åˆ›å»ºESP-netæ ‡å‡†æ ¼å¼æ–‡ä»¶
        with open(split_dir / "speech.scp", 'w') as scp_f, \\
             open(split_dir / "emotion.txt", 'w') as emo_f, \\
             open(split_dir / "utt2spk", 'w') as spk_f:
            
            for utt_id, info in data.items():
                wav_path = info['wav']
                emotion = info['emo']
                duration = info.get('length', 0)
                
                # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨ä¸”æ ‡ç­¾æœ‰æ•ˆ
                if not os.path.exists(wav_path):
                    missing_files.append(wav_path)
                    continue
                    
                if emotion not in emotion_map:
                    print(f"âš ï¸  Unknown emotion '{emotion}' for {utt_id}")
                    continue
                
                # å†™å…¥ESP-netæ ¼å¼æ–‡ä»¶
                scp_f.write(f"{utt_id} {wav_path}\\n")
                emo_f.write(f"{utt_id} {emotion_map[emotion]}\\n")
                
                # ç®€å•çš„speaker ID (ä»utterance IDæå–)
                speaker_id = '_'.join(utt_id.split('_')[:2])
                spk_f.write(f"{utt_id} {speaker_id}\\n")
                
                valid_count += 1
                durations.append(duration)
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats[split] = {
            'count': valid_count,
            'missing_files': len(missing_files),
            'duration_mean': np.mean(durations) if durations else 0,
            'duration_std': np.std(durations) if durations else 0,
            'duration_min': np.min(durations) if durations else 0,
            'duration_max': np.max(durations) if durations else 0,
            'emotion_dist': emotion_counts
        }
        
        print(f"âœ… {split}: {valid_count} valid samples")
        if missing_files:
            print(f"âš ï¸  {split}: {len(missing_files)} missing files")
            # æ˜¾ç¤ºå‰å‡ ä¸ªç¼ºå¤±æ–‡ä»¶ä½œä¸ºç¤ºä¾‹
            for i, missing in enumerate(missing_files[:3]):
                print(f"     Missing: {missing}")
            if len(missing_files) > 3:
                print(f"     ... and {len(missing_files) - 3} more")
    
    # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
    dataset_info = {
        "dataset_name": "MSP-PODCAST",
        "task": "10-class emotion recognition",
        "num_classes": 10,
        "emotion_names": emotion_names,
        "emotion_mapping": emotion_map,
        "stats": stats,
        "data_source": "Cloud cluster: /data/user_data/esthers/SER_MSP"
    }
    
    with open(output_path / "dataset_info.json", 'w') as f:
        json.dump(dataset_info, f, indent=2, ensure_ascii=False)
    
    print(f"\\nâœ… ESP-net data prepared in {output_dir}/")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
    print("\\nğŸ“Š Dataset Statistics:")
    print(f"Dataset: {dataset_info['dataset_name']}")
    print(f"Task: {dataset_info['task']}")
    print(f"Classes: {dataset_info['num_classes']}")
    
    for split in ['train', 'valid', 'test']:
        if split in stats:
            print(f"\\n{split.upper()} Set:")
            print(f"  Samples: {stats[split]['count']:,}")
            print(f"  Missing files: {stats[split]['missing_files']}")
            print(f"  Duration: {stats[split]['duration_mean']:.2f}Â±{stats[split]['duration_std']:.2f}s")
            print(f"  Range: [{stats[split]['duration_min']:.2f}, {stats[split]['duration_max']:.2f}]s")
            
            print(f"  Emotion distribution:")
            total = stats[split]['count']
            for emotion, count in stats[split]['emotion_dist'].items():
                pct = count / total * 100 if total > 0 else 0
                emotion_name = emotion_names[emotion_map[emotion]]
                print(f"    {emotion} ({emotion_name}): {count:,} ({pct:.1f}%)")
    
    # ç±»åˆ«å¹³è¡¡åˆ†æ
    if 'train' in stats:
        print(f"\\nâš–ï¸  Class Balance Analysis (Training Set):")
        train_dist = stats['train']['emotion_dist']
        counts = list(train_dist.values())
        if counts:
            max_count = max(counts)
            min_count = min(counts)
            balance_ratio = min_count / max_count
            print(f"  Balance ratio: {balance_ratio:.3f} (1.0 = perfect)")
            
            if balance_ratio < 0.5:
                print(f"  âš ï¸  Severe class imbalance detected!")
                print(f"     Consider using class weights or data resampling")
    
    return dataset_info

if __name__ == "__main__":
    prepare_cloud_msp_data()
'''
    
    with open("cloud_data_prep.py", 'w') as f:
        f.write(data_prep_code)
    
    print("âœ… Created cloud_data_prep.py")

def create_cloud_train_config():
    """åˆ›å»ºäº‘ç«¯è®­ç»ƒé…ç½®"""
    
    config = '''# cloud_train_config.yaml - äº‘ç«¯é›†ç¾¤è®­ç»ƒé…ç½®
batch_type: numel
batch_size: 12                      # äº‘ç«¯å¯èƒ½æœ‰æ›´å¥½çš„GPU
max_epoch: 50
patience: 10
seed: 42
num_workers: 4                      # äº‘ç«¯å¹¶è¡Œå¤„ç†
log_interval_steps: 100
grad_clip: 5.0
accum_grad: 1

# ä¼˜åŒ–å™¨é…ç½®
optim: adamw
optim_conf:
  lr: 0.0001
  weight_decay: 0.001
  betas: [0.9, 0.999]
  eps: 1.0e-8

# å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler: cosineannealinglr
scheduler_conf:
  T_max: 50
  eta_min: 1.0e-6

# æ¨¡å‹é…ç½® (é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ä¼ é€’)
num_class: 10
wavlm_model_name: "microsoft/wavlm-large"
wavlm_freeze: true
ecapa_channels: [512, 512, 512]
ecapa_kernels: [5, 3, 3]
ecapa_dilations: [1, 2, 3]
context_dim: 1536
embedding_dim: 256
loss_type: "cross_entropy"
focal_gamma: 2.0
label_smoothing: 0.1
save_macro_f1: true

# é¢„å¤„ç†å™¨é…ç½®
preprocessor: default
preprocessor_conf:
  # SpecAugment
  spec_augment: true
  spec_augment_conf:
    apply_time_warp: true
    time_warp_window: 5
    apply_freq_mask: true
    freq_mask_width_range: [0, 30]
    num_freq_mask: 2
    apply_time_mask: true
    time_mask_width_range: [0, 40]
    num_time_mask: 2

# æœ€ä½³æ¨¡å‹é€‰æ‹©ï¼ˆé‡ç‚¹å…³æ³¨macro-F1ï¼‰
best_model_criterion:
  - ["valid", "macro_f1", "max"]
  - ["valid", "acc", "max"]
  - ["valid", "loss", "min"]

# å…¶ä»–é…ç½®
resume: true
keep_nbest_models: 5
use_tensorboard: true
'''
    
    with open("cloud_train_config.yaml", 'w') as f:
        f.write(config)
    
    print("âœ… Created cloud_train_config.yaml")

def check_cloud_dependencies():
    """æ£€æŸ¥äº‘ç«¯ä¾èµ–"""
    print("ğŸ“¦ Checking cloud dependencies...")
    
    required_packages = {
        'espnet': 'espnet',
        'transformers': 'transformers', 
        'sklearn': 'scikit-learn',
        'torch': 'torch',
        'torchaudio': 'torchaudio',
        'yaml': 'pyyaml',
        'numpy': 'numpy'
    }
    
    missing = []
    for import_name, package_name in required_packages.items():
        try:
            __import__(import_name)
            print(f"âœ… {package_name}")
        except ImportError:
            missing.append(package_name)
            print(f"âŒ {package_name}")
    
    if missing:
        print(f"\nğŸ”§ Installing missing packages...")
        install_cmd = [sys.executable, "-m", "pip", "install"] + missing
        print(f"Command: {' '.join(install_cmd)}")
        
        try:
            subprocess.run(install_cmd, check=True)
            print("âœ… Dependencies installed successfully")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to install dependencies: {e}")
            return False
    
    return True

def ensure_cloud_scripts():
    """ç¡®ä¿äº‘ç«¯è„šæœ¬å­˜åœ¨"""
    # ç¡®ä¿æ³¨å†Œè„šæœ¬å·²è¿è¡Œ
    register_script = Path("register_model.py")
    ser_train_script = Path("ser_train.py")
    
    if not ser_train_script.exists():
        print("ğŸ”§ Creating necessary scripts...")
        if register_script.exists():
            try:
                subprocess.run([sys.executable, "register_model.py"], check=True)
            except subprocess.CalledProcessError as e:
                print(f"âŒ Failed to run register_model.py: {e}")
                return False
        else:
            print("âŒ register_model.py not found!")
            return False
    
    return True

def run_cloud_training():
    """åœ¨äº‘ç«¯è¿è¡Œè®­ç»ƒ"""
    print("\nğŸš€ Starting cloud training...")
    
    # 1. æ•°æ®å‡†å¤‡
    print("ğŸ“Š Step 1: Data preparation...")
    try:
        from cloud_data_prep import prepare_cloud_msp_data
        dataset_info = prepare_cloud_msp_data()
        
        # æ£€æŸ¥æ•°æ®å‡†å¤‡ç»“æœ
        data_dir = Path("data")
        total_samples = 0
        for split in ["train", "valid", "test"]:
            speech_scp = data_dir / split / "speech.scp"
            emotion_txt = data_dir / split / "emotion.txt"
            
            if speech_scp.exists() and emotion_txt.exists():
                with open(speech_scp, 'r') as f:
                    samples = len(f.readlines())
                total_samples += samples
                print(f"  {split}: {samples} samples")
        
        if total_samples == 0:
            print("âŒ No valid samples found after data preparation")
            return False
        
        print(f"âœ… Total samples prepared: {total_samples}")
        
    except Exception as e:
        print(f"âŒ Data preparation failed: {e}")
        return False
    
    # 2. æ£€æŸ¥è„šæœ¬
    print("\nğŸ”§ Step 2: Ensuring scripts...")
    if not ensure_cloud_scripts():
        print("âŒ Script preparation failed")
        return False
    
    # 3. æ¨¡å‹è®­ç»ƒ
    print("\nğŸš€ Step 3: Model training...")
    
    exp_dir = Path("exp/cloud_wavlm_ecapa")
    exp_dir.mkdir(parents=True, exist_ok=True)
    
    # ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„SERè®­ç»ƒè„šæœ¬
    train_cmd = [
        sys.executable, "ser_train.py",
        "--config", "cloud_train_config.yaml",
        "--train_data_path_and_name_and_type", "data/train/speech.scp,speech,sound",
        "--train_data_path_and_name_and_type", "data/train/emotion.txt,emotion,text",
        "--valid_data_path_and_name_and_type", "data/valid/speech.scp,speech,sound", 
        "--valid_data_path_and_name_and_type", "data/valid/emotion.txt,emotion,text",
        "--output_dir", str(exp_dir),
        "--ngpu", "1",
        "--num_workers", "4",
        "--use_preprocessor", "true",
    ]
    
    print("Training command:")
    print(" ".join(train_cmd))
    print()
    
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['PYTHONPATH'] = str(Path.cwd()) + ':' + env.get('PYTHONPATH', '')
        
        result = subprocess.run(train_cmd, check=True, env=env)
        print("âœ… Training completed successfully!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Training failed: {e}")
        return False
    except FileNotFoundError as e:
        print(f"âŒ Script not found: {e}")
        return False

def run_cloud_evaluation():
    """è¿è¡Œäº‘ç«¯è¯„ä¼°"""
    print("\nğŸ“Š Step 4: Model evaluation...")
    
    exp_dir = Path("exp/cloud_wavlm_ecapa")
    
    # æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶
    model_patterns = [
        "valid.macro_f1.best.pth",
        "valid.acc.best.pth",
        "valid.loss.best.pth",
        "checkpoint.pth",
    ]
    
    model_file = None
    for pattern in model_patterns:
        candidate = exp_dir / pattern
        if candidate.exists():
            model_file = candidate
            break
    
    if model_file is None:
        print("âŒ No trained model found")
        print("Available files in experiment directory:")
        if exp_dir.exists():
            for f in exp_dir.iterdir():
                if f.suffix == '.pth':
                    print(f"  - {f}")
        return False
    
    print(f"ğŸ“Š Using model: {model_file}")
    
    # æ£€æŸ¥æµ‹è¯•æ•°æ®
    test_data_dir = Path("data/test")
    if not test_data_dir.exists() or not (test_data_dir / "speech.scp").exists():
        print("âš ï¸  Test data not found, using validation data for evaluation")
        test_data_dir = Path("data/valid")
    
    # è¿è¡Œè¯„ä¼°
    eval_cmd = [
        sys.executable, "ser_inference.py",
        "--model_file", str(model_file),
        "--valid_data_path_and_name_and_type", f"{test_data_dir}/speech.scp,speech,sound",
        "--valid_data_path_and_name_and_type", f"{test_data_dir}/emotion.txt,emotion,text",
        "--output_dir", str(exp_dir / "evaluation"),
        "--ngpu", "1",
        "--batch_size", "32",
    ]
    
    print("Evaluation command:")
    print(" ".join(eval_cmd))
    print()
    
    try:
        result = subprocess.run(eval_cmd, check=True)
        print("âœ… Evaluation completed successfully!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Evaluation failed: {e}")
        return False
    except FileNotFoundError:
        print("âš ï¸  Evaluation script not found, skipping evaluation")
        return True  # ä¸ç®—å¤±è´¥

def main():
    print("ğŸŒ Cloud Cluster ESP-net WavLM + ECAPA-TDNN")
    print("=" * 60)
    print(f"ğŸ“ Data root: {DATA_ROOT}")
    print(f"ğŸµ Audio dir: {AUDIO_DIR}")
    print("=" * 60)
    
    # 1. ç¯å¢ƒè®¾ç½®
    print("\nğŸ”§ Setting up cloud environment...")
    setup_cloud_environment()
    
    # 2. æ£€æŸ¥æ•°æ®
    print("\nğŸ“Š Checking cloud data...")
    if not check_cloud_data():
        print("âŒ Cloud data check failed")
        print("Please check your data paths and JSON files")
        return
    
    # 3. æ£€æŸ¥ä¾èµ–
    print("\nğŸ“¦ Checking dependencies...")
    if not check_cloud_dependencies():
        print("âŒ Dependency check failed")
        return
    
    # 4. åˆ›å»ºäº‘ç«¯é…ç½®æ–‡ä»¶
    print("\nğŸ”§ Creating cloud configuration...")
    create_cloud_data_prep()
    create_cloud_train_config()
    
    # 5. è¿è¡Œå®Œæ•´æµç¨‹
    try:
        # æ•°æ®å‡†å¤‡å’Œè®­ç»ƒ
        if not run_cloud_training():
            print("\nâŒ Cloud training failed")
            return
        
        # è¯„ä¼°
        run_cloud_evaluation()
        
        print("\nğŸ‰ Cloud training completed successfully!")
        print("\nğŸ“Š Results location:")
        print(f"  - Model: exp/cloud_wavlm_ecapa/")
        print(f"  - Logs: exp/cloud_wavlm_ecapa/train.log")
        print(f"  - TensorBoard: exp/cloud_wavlm_ecapa/tensorboard/")
        print(f"  - Data info: data/dataset_info.json")
        
        # æ˜¾ç¤ºè®­ç»ƒç»“æœæ‘˜è¦
        exp_dir = Path("exp/cloud_wavlm_ecapa")
        log_file = exp_dir / "train.log"
        if log_file.exists():
            print(f"\nğŸ“ˆ Check training progress:")
            print(f"  tail -f {log_file}")
        
        tensorboard_dir = exp_dir / "tensorboard"
        if tensorboard_dir.exists():
            print(f"\nğŸ“Š View TensorBoard:")
            print(f"  tensorboard --logdir {tensorboard_dir}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  Training interrupted by user")
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")

if __name__ == "__main__":
    main()